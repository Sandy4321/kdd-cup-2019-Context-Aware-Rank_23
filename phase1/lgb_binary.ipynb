{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score\n",
    "from time import gmtime, strftime\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_profile_data():\n",
    "    profile_data = pd.read_csv(dir_path + 'profiles.csv')\n",
    "    profile_na = np.zeros(67)-1\n",
    "    profile_na = pd.DataFrame(profile_na.reshape(1, -1))\n",
    "    profile_na.columns = profile_data.columns\n",
    "    profile_data = profile_data.append(profile_na)\n",
    "    return profile_data\n",
    "\n",
    "\n",
    "def merge_raw_data():\n",
    "    te_plans = pd.read_csv(dir_path + 'test_plans.csv')\n",
    "    te_queries = pd.read_csv(dir_path + 'test_queries.csv')\n",
    "\n",
    "    tr_click = pd.read_csv(dir_path + 'train_clicks.csv')\n",
    "    tr_plans = pd.read_csv(dir_path + 'train_plans.csv')\n",
    "    tr_queries = pd.read_csv(dir_path + 'train_queries.csv')\n",
    "\n",
    "    tr_data = tr_queries.merge(tr_click, on='sid', how='left')\n",
    "    tr_data = tr_data.merge(tr_plans, on='sid', how='left')\n",
    "    tr_data = tr_data.drop(['click_time'], axis=1)\n",
    "    tr_data['click_mode'] = tr_data['click_mode'].fillna(0)\n",
    "\n",
    "    te_data = te_queries.merge(te_plans, on='sid', how='left')\n",
    "    te_data['click_mode'] = -1\n",
    "\n",
    "    data = pd.concat([tr_data, te_data], axis=0)\n",
    "#     data = data.drop(['plan_time'], axis=1)\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    print('total data size: {}'.format(data.shape))\n",
    "    print('raw data columns: {}'.format(', '.join(data.columns)))\n",
    "    return data\n",
    "\n",
    "\n",
    "def gen_od_feas(data):\n",
    "    def GetDistance(lng1, lat1, lng2, lat2):\n",
    "        EARTH_RADIUS = 6378.137\n",
    "\n",
    "        lng1 = lng1*math.pi / 180.0\n",
    "        lng2 = lng2*math.pi / 180.0\n",
    "        lat1 = lat1*math.pi / 180.0\n",
    "        lat2 = lat2*math.pi / 180.0\n",
    "\n",
    "        dis1 = lat1-lat2\n",
    "        dis2 = lng1-lng2\n",
    "\n",
    "        s = 2*math.asin( ((math.sin(dis1/2))**2 + math.cos(lat1)*math.cos(lat2)*(math.sin(dis2/2))**2)**0.5 )\n",
    "        s = s * EARTH_RADIUS  * 1000\n",
    "        return s\n",
    "    \n",
    "#     subway = pd.read_csv('../data/other_data/beijing_subway.csv')\n",
    "#     bus_station = pd.read_csv('../data/other_data/beijing_bus_station.csv')\n",
    "    \n",
    "    data['o1'] = data['o'].apply(lambda x: float(x.split(',')[0]))\n",
    "    data['o2'] = data['o'].apply(lambda x: float(x.split(',')[1]))\n",
    "    data['d1'] = data['d'].apply(lambda x: float(x.split(',')[0]))\n",
    "    data['d2'] = data['d'].apply(lambda x: float(x.split(',')[1]))\n",
    "    data['dist'] = data.apply(lambda line:((line['o1']-line['d1'])**2 + (line['o2']-line['d2'])**2)**(0.5),axis=1)\n",
    "    data['real_dis'] = data.apply(lambda row:GetDistance(row['o1'], row['o2'], row['d1'], row['d2']), axis=1)\n",
    "    data['real_dis_60000'] = data['real_dis'].apply(lambda x:1 if x>=60000 else 0)\n",
    "    data['real_dis_10000'] = data['real_dis'].apply(lambda x:1 if x<=10000 else 0)\n",
    "    data['real_dis_7500'] = data['real_dis'].apply(lambda x:1 if x<=7500 else 0)\n",
    "    \n",
    "    data['real_dis_2500'] = data['real_dis'].apply(lambda x:1 if x<=2500 else 0)\n",
    "    data['od_manhattan_distance'] = abs(data['o1']-data['d1'])+abs(data['o2']-data['d2'])\n",
    "    \n",
    "#     data['o_nearest_sub'] = data.apply(lambda row:(abs(subway['station_longitude']-row['o1'])\n",
    "#                                       +abs(subway['station_latitude']-row['o2'])).min(), axis=1)\n",
    "\n",
    "#     data['d_nearest_sub'] = data.apply(lambda row:(abs(subway['station_longitude']-row['d1'])\n",
    "#                                       +abs(subway['station_latitude']-row['d2'])).min(), axis=1)\n",
    "    \n",
    "#     data['d_nearest_bus'] = data.apply(lambda row:(abs(bus_station['lng']-row['o1'])\n",
    "#                                       +abs(bus_station['lat']-row['o2'])).min(), axis=1)\n",
    "#     data['d_nearest_bus'] = data.apply(lambda row:(abs(bus_station['lng']-row['d1'])\n",
    "#                                       +abs(bus_station['lat']-row['d2'])).min(), axis=1)\n",
    "\n",
    "    data = data.drop(['o', 'd'], axis=1)\n",
    "    return data\n",
    "\n",
    "\n",
    "def gen_profile_feas(data):\n",
    "    profile_data = read_profile_data()\n",
    "    p_feat = ['p'+str(num) for num in range(66)]\n",
    "    profile_data[p_feat] = profile_data[p_feat].astype(int).astype(str)\n",
    "#     data['pid'] = data['pid'].fillna(-1)\n",
    "    profile_data = pd.get_dummies(profile_data)\n",
    "    data = data.merge(profile_data, on='pid', how='left')\n",
    "    \n",
    "#     x = profile_data.drop(['pid'], axis=1).values\n",
    "#     svd = TruncatedSVD(n_components=20, n_iter=20, random_state=2019)\n",
    "#     svd_x = svd.fit_transform(x)\n",
    "#     svd_feas = pd.DataFrame(svd_x)\n",
    "#     svd_feas.columns = ['svd_fea_{}'.format(i) for i in range(20)]\n",
    "#     svd_feas['pid'] = profile_data['pid'].values\n",
    "#     data = data.merge(svd_feas, on='pid', how='left')\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def gen_time_feas(data):\n",
    "    data['req_time'] = pd.to_datetime(data['req_time'])\n",
    "    data['plan_time'] = pd.to_datetime(data['plan_time'])\n",
    "    data['time_diff'] = data['plan_time'].astype(int) - data['req_time'].astype(int)\n",
    "    \n",
    "    data['weekday'] = data['req_time'].dt.dayofweek\n",
    "    data['IsWeek'] = data['weekday'].apply(lambda x:1 if x > 5 else 0).astype(str)\n",
    "    data['hour'] = data['req_time'].dt.hour\n",
    "    data['minute'] = data['req_time'].dt.minute\n",
    "    \n",
    "    data['hour_3_23'] = data['hour'].apply(lambda x:1 if x>=3 & x <= 23 else 0)\n",
    "    data['hour_5_23'] = data['hour'].apply(lambda x:1 if x>=5 & x <= 23 else 0)\n",
    "    data['hour_minute'] = data['hour']*60 + data['minute']\n",
    "    \n",
    "#     data['weekday'] = data['req_time'].astype(str)\n",
    "#     data['hour'] = data['hour'].astype(str)\n",
    "#     data = pd.concat([data,pd.get_dummies(data[['IsWeek','weekday','hour']])], axis=1)\n",
    "    data = pd.concat([data,pd.get_dummies(data[['IsWeek']])], axis=1)\n",
    "    data = data.drop(['plan_time'], axis=1)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def gen_plan_feas(data):\n",
    "    def num_plans(plan):\n",
    "        try:\n",
    "            cur_plan_list = json.loads(plan)\n",
    "        except:\n",
    "            cur_plan_list = []\n",
    "        return len(cur_plan_list)\n",
    "\n",
    "    n = data.shape[0]\n",
    "    mode_list_feas = np.zeros((n, 12))\n",
    "    max_dist, min_dist, mean_dist, std_dist = \\\n",
    "        np.zeros((n,)), np.zeros((n,)), np.zeros((n,)), np.zeros((n,))\n",
    "\n",
    "    max_price, min_price, mean_price, std_price = \\\n",
    "        np.zeros((n,)), np.zeros((n,)), np.zeros((n,)), np.zeros((n,))\n",
    "\n",
    "    max_eta, min_eta, mean_eta, std_eta = \\\n",
    "        np.zeros((n,)), np.zeros((n,)), np.zeros((n,)), np.zeros((n,))\n",
    "    \n",
    "    max_price_eta, min_price_eta, mean_price_eta, std_price_eta = \\\n",
    "        np.zeros((n,)), np.zeros((n,)), np.zeros((n,)), np.zeros((n,))\n",
    "    \n",
    "    min_dist_mode, max_dist_mode, min_price_mode, max_price_mode = \\\n",
    "        np.zeros((n,)), np.zeros((n,)), np.zeros((n,)), np.zeros((n,))\n",
    "        \n",
    "    min_eta_mode, max_eta_mode, min_price_eta_mode, max_price_eta_mode, first_mode = \\\n",
    "        np.zeros((n,)),np.zeros((n,)), np.zeros((n,)), np.zeros((n,)), np.zeros((n,))\n",
    "    \n",
    "    mode_texts = []\n",
    "\n",
    "    for i, plan in tqdm(enumerate(data['plans'].values)):\n",
    "        try:\n",
    "            cur_plan_list = json.loads(plan)\n",
    "        except:\n",
    "            cur_plan_list = []\n",
    "\n",
    "        if len(cur_plan_list) == 0:\n",
    "            mode_list_feas[i, 0] = 1\n",
    "            first_mode[i] = 0\n",
    "\n",
    "            max_dist[i] = -1\n",
    "            min_dist[i] = -1\n",
    "            mean_dist[i] = -1\n",
    "            std_dist[i] = -1\n",
    "\n",
    "            max_price[i] = -1\n",
    "            min_price[i] = -1\n",
    "            mean_price[i] = -1\n",
    "            std_price[i] = -1\n",
    "\n",
    "            max_eta[i] = -1\n",
    "            min_eta[i] = -1\n",
    "            mean_eta[i] = -1\n",
    "            std_eta[i] = -1\n",
    "\n",
    "            min_dist_mode[i] = -1\n",
    "            max_dist_mode[i] = -1\n",
    "            min_price_mode[i] = -1\n",
    "            max_price_mode[i] = -1\n",
    "            min_eta_mode[i] = -1\n",
    "            max_eta_mode[i] = -1\n",
    "\n",
    "            mode_texts.append('word_null')\n",
    "\n",
    "        else:\n",
    "            distance_list = []\n",
    "            price_list = []\n",
    "            eta_list = []\n",
    "            price_eta_list = []\n",
    "            mode_list = []\n",
    "            \n",
    "            for tmp_dit in cur_plan_list:\n",
    "                distance_list.append(int(tmp_dit['distance']))\n",
    "                if tmp_dit['price'] == '':\n",
    "                    if tmp_dit['transport_mode'] == 3:\n",
    "                        temp_price = 10000\n",
    "                    else:\n",
    "                        temp_price = 0\n",
    "                else:\n",
    "                    temp_price = int(tmp_dit['price'])\n",
    "                    \n",
    "                price_list.append(temp_price)\n",
    "                price_eta_list.append(int(tmp_dit['eta']) * int(temp_price))\n",
    "                eta_list.append(int(tmp_dit['eta']))\n",
    "                mode_list.append(int(tmp_dit['transport_mode']))\n",
    "\n",
    "            mode_texts.append(' '.join(['word_{}'.format(mode) for mode in mode_list]))\n",
    "                        \n",
    "            # 保存成array\n",
    "            distance_list = np.array(distance_list)\n",
    "            price_list = np.array(price_list)\n",
    "            eta_list = np.array(eta_list)\n",
    "            price_eta_list = np.array(price_eta_list)\n",
    "            mode_list = np.array(mode_list, dtype='int')\n",
    "            \n",
    "            mode_list_feas[i, mode_list] = 1\n",
    "            \n",
    "            # 对array进行排序然后得到其index\n",
    "            distance_sort_idx = np.argsort(distance_list)\n",
    "            price_sort_idx = np.argsort(price_list)\n",
    "            eta_sort_idx = np.argsort(eta_list)\n",
    "            price_eta_sort_idx = np.argsort(price_eta_list)\n",
    "            \n",
    "            # 求dist的最大、小、均值，标准差\n",
    "            max_dist[i] = distance_list[distance_sort_idx[-1]]\n",
    "            min_dist[i] = distance_list[distance_sort_idx[0]]\n",
    "            mean_dist[i] = np.mean(distance_list)\n",
    "            std_dist[i] = np.std(distance_list)\n",
    "                \n",
    "            # 求price的最大、小、均值，标准差\n",
    "            max_price[i] = price_list[price_sort_idx[-1]]\n",
    "            min_price[i] = price_list[price_sort_idx[0]]\n",
    "            mean_price[i] = np.mean(price_list)\n",
    "            std_price[i] = np.std(price_list)\n",
    "            \n",
    "            # 求eta的最大、小、均值，标准差\n",
    "            max_eta[i] = eta_list[eta_sort_idx[-1]]\n",
    "            min_eta[i] = eta_list[eta_sort_idx[0]]\n",
    "            mean_eta[i] = np.mean(eta_list)\n",
    "            std_eta[i] = np.std(eta_list)\n",
    "            \n",
    "            # 求price*eta的最大、小、均值，标准差\n",
    "            max_price_eta[i] = price_eta_list[price_eta_sort_idx[-1]]\n",
    "            min_price_eta[i] = price_eta_list[price_eta_sort_idx[0]]\n",
    "            mean_price_eta[i] = np.mean(price_eta_list)\n",
    "            std_price_eta[i] = np.std(price_eta_list)\n",
    "            \n",
    "            # 求第一个mode\n",
    "            first_mode[i] = mode_list[0]\n",
    "            \n",
    "            # 求dist最大最小时的mode\n",
    "            max_dist_mode[i] = mode_list[distance_sort_idx[-1]]\n",
    "            min_dist_mode[i] = mode_list[distance_sort_idx[0]]\n",
    "            \n",
    "            # 求price最大最小时的mode\n",
    "            max_price_mode[i] = mode_list[price_sort_idx[-1]]\n",
    "            min_price_mode[i] = mode_list[price_sort_idx[0]]\n",
    "            \n",
    "            # 求eta最大最小时的mode\n",
    "            max_eta_mode[i] = mode_list[eta_sort_idx[-1]]\n",
    "            min_eta_mode[i] = mode_list[eta_sort_idx[0]]\n",
    "            \n",
    "            # 求price×eta最大最小时的mode\n",
    "            max_price_eta_mode[i] = mode_list[price_eta_sort_idx[-1]]\n",
    "            min_price_eta_mode[i] = mode_list[price_eta_sort_idx[0]]\n",
    "\n",
    "    feature_data = pd.DataFrame(mode_list_feas)\n",
    "    feature_data.columns = ['mode_feas_{}'.format(i) for i in range(12)]\n",
    "    feature_data['max_dist'] = max_dist\n",
    "    feature_data['min_dist'] = min_dist\n",
    "    feature_data['mean_dist'] = mean_dist\n",
    "    feature_data['std_dist'] = std_dist\n",
    "\n",
    "    feature_data['max_price'] = max_price\n",
    "    feature_data['min_price'] = min_price\n",
    "    feature_data['mean_price'] = mean_price\n",
    "    feature_data['std_price'] = std_price\n",
    "\n",
    "    feature_data['max_eta'] = max_eta\n",
    "    feature_data['min_eta'] = min_eta\n",
    "    feature_data['mean_eta'] = mean_eta\n",
    "    feature_data['std_eta'] = std_eta\n",
    "    \n",
    "    feature_data['max_price_eta'] = max_price_eta\n",
    "    feature_data['min_price_eta'] = min_price_eta\n",
    "    feature_data['mean_price_eta'] = mean_price_eta\n",
    "    feature_data['std_price_eta'] = std_price_eta\n",
    "\n",
    "    feature_data['max_dist_mode'] = max_dist_mode\n",
    "    feature_data['min_dist_mode'] = min_dist_mode\n",
    "    \n",
    "    feature_data['max_price_mode'] = max_price_mode\n",
    "    feature_data['min_price_mode'] = min_price_mode\n",
    "    \n",
    "    feature_data['max_eta_mode'] = max_eta_mode\n",
    "    feature_data['min_eta_mode'] = min_eta_mode\n",
    "    \n",
    "    feature_data['max_price_eta_mode'] = max_price_eta_mode\n",
    "    feature_data['min_price_eta_mode'] = min_price_eta_mode\n",
    "    feature_data['first_mode'] = first_mode\n",
    "\n",
    "    print('mode tfidf...')\n",
    "    tfidf_enc = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    tfidf_vec = tfidf_enc.fit_transform(mode_texts)\n",
    "    svd_enc = TruncatedSVD(n_components=10, n_iter=20, random_state=2019)\n",
    "    mode_svd = svd_enc.fit_transform(tfidf_vec)\n",
    "    mode_svd = pd.DataFrame(mode_svd)\n",
    "    mode_svd.columns = ['svd_mode_{}'.format(i) for i in range(10)]\n",
    "    data = pd.concat([data, feature_data, mode_svd], axis=1)\n",
    "\n",
    "    data['nums_plans'] = data['plans'].apply(num_plans)\n",
    "    data = data.drop(['plans'], axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data size: (594358, 8)\n",
      "raw data columns: click_mode, d, o, pid, plan_time, plans, req_time, sid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "594358it [02:10, 4545.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode tfidf...\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../data/data_set_phase1/'\n",
    "data = merge_raw_data()\n",
    "data = gen_od_feas(data)\n",
    "data = gen_time_feas(data)\n",
    "\n",
    "data = gen_plan_feas(data)\n",
    "data = gen_profile_feas(data)\n",
    "bus_subway = pd.read_csv('../data/other_data/feature_subway_rela.csv')\n",
    "data = data.merge(bus_subway, on='sid',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid_feat = ['p'+str(num) for num in range(66)]\n",
    "\n",
    "time_feature_cate = ['day','weekday','IsWeek','hour','hour_3_23','hour_5_23']\n",
    "time_feature_num = ['minute','time_diff','hour_minute']\n",
    "\n",
    "od_feat_num = ['o1','o2','d1','d2','dist','real_dis','od_manhattan_distance']\n",
    "od_feat_cate = ['o','d','real_dis_60000','real_dis_10000','real_dis_7500']\n",
    "\n",
    "plan_feat_cate = ['min_dist_mode', 'max_dist_mode', 'min_price_mode', 'max_price_mode',\n",
    "                  'min_eta_mode', 'max_eta_mode', 'first_mode'] + ['mode_feas_{}'.format(i) for i in range(12)]\n",
    "\n",
    "plan_feat_num = ['max_dist', 'min_dist', 'mean_dist', 'std_dist',\n",
    "                 'max_price', 'min_price', 'mean_price', 'std_price',\n",
    "                 'max_eta', 'min_eta', 'mean_eta', 'std_eta','nums_plans','nums_plans']\n",
    "\n",
    "plan_feat_svd = ['svd_mode_{}'.format(i) for i in range(10)]\n",
    "\n",
    "num_feat = time_feature_num + od_feat_num + plan_feat_num\n",
    "cate_feat = time_feature_cate + plan_feat_cate + ['pid'] + ['sid']\n",
    "\n",
    "feature = [col for col in data.columns if col not in ['click_mode','req_time','sid','IsWeek']]\n",
    "\n",
    "# minmax-scaler\n",
    "scaler = MinMaxScaler()\n",
    "data[num_feat] = scaler.fit_transform(data[num_feat])\n",
    "\n",
    "# Z-scaler\n",
    "# scaler = StandardScaler()\n",
    "# data[num_feat] = scaler.fit_transform(data[num_feat])\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# ohEncoder = OneHotEncoder()\n",
    "# ohEncoder.fit_transform(data[['hour','IsWeek']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 2.22 s, total: 3.7 s\n",
      "Wall time: 3.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_index = (data.req_time < '2018-11-23')\n",
    "train_x     = data[train_index][feature].reset_index(drop=True)\n",
    "train_y     = data[train_index].click_mode.reset_index(drop=True)\n",
    "\n",
    "valid_index = (data.req_time > '2018-11-23') & (data.req_time < '2018-12-01')\n",
    "valid_x     = data[valid_index][feature].reset_index(drop=True)\n",
    "valid_y     = data[valid_index].click_mode.reset_index(drop=True)\n",
    "\n",
    "output = pd.DataFrame()\n",
    "output['sid']   = data[valid_index]['sid']\n",
    "output['label'] = valid_y.values\n",
    "dic_ = output['label'].value_counts(normalize = True)\n",
    "\n",
    "test_index = (data.req_time > '2018-12-01')\n",
    "test_x     = data[test_index][feature].reset_index(drop=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_weighted(labels,preds):\n",
    "    print(preds)\n",
    "    preds = np.argmax(preds, axis=0)\n",
    "    score = f1_score(y_true=labels, y_pred=preds)\n",
    "    return 'f1_weighted', score, True\n",
    "\n",
    "\n",
    "def lgb_train_binary(train_x, train_y, valid_x, valid_y,f_score):\n",
    "\n",
    "    lgb_model = lgb.LGBMClassifier(boosting_type=\"gbdt\", num_leaves=120, reg_alpha=0, reg_lambda=0,\n",
    "                                   max_depth=-1, n_estimators=2000, objective='binary',\n",
    "                                   learning_rate=0.03, random_state=123, metric=\"binary_logloss\",n_jobs=-1,\n",
    "                                   subsample=0.8, colsample_bytree=0.8, subsample_freq=1, min_child_samples = 50,)\n",
    "\n",
    "    eval_set = [(train_x, train_y),(valid_x, valid_y)]\n",
    "    lgb_model.fit(train_x, train_y, eval_set=eval_set, eval_metric='logloss', \n",
    "                  verbose=50, early_stopping_rounds=200)\n",
    "    \n",
    "    valid_prob_y = lgb_model.predict_proba(valid_x)[:,1]\n",
    "    \n",
    "    output[str(mode)] = valid_prob_y\n",
    "    \n",
    "    best_f1_ = 0\n",
    "    best_thresh = 0\n",
    "    \n",
    "    for thresh in range(3,9):\n",
    "        thresh *= 0.1\n",
    "        valid_pred_y = valid_prob_y >= thresh\n",
    "        f1_ = f1_score(y_true=valid_y, y_pred=valid_pred_y)\n",
    "        if f1_ > best_f1_:\n",
    "            best_f1_ = f1_\n",
    "            best_thresh = thresh\n",
    "            \n",
    "    valid_pred_y = valid_prob_y >= best_thresh\n",
    "    weighted_f1 = dic_[mode] * f1_\n",
    "    f_score += weighted_f1\n",
    "    precision_ = precision_score(y_true=valid_y, y_pred=valid_pred_y)\n",
    "    recall_ = recall_score(y_true=valid_y, y_pred=valid_pred_y)\n",
    "\n",
    "    print(f'mode:{mode},  F1:{best_f1_:.6f}, weighted_f1:{weighted_f1:.6f}, ratio:{dic_[mode]:.6f}, Precision:{precision_:.6f}, Recall:{recall_:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5491a7031b1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtemp_train_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtemp_valid_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalid_y\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mlgb_train_binary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_train_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalid_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_valid_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mf_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_y' is not defined"
     ]
    }
   ],
   "source": [
    "f_score = 0 \n",
    "for mode in range(12):\n",
    "    temp_train_y = train_y == mode\n",
    "    temp_valid_y = valid_y == mode\n",
    "    lgb_train_binary(train_x, temp_train_y, valid_x, temp_valid_y,f_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mode:0,  F1:0.347218, weighted_f1:0.030352, ratio:0.087414, Precision:0.899623, Recall:0.215124\n",
    "mode:1,  F1:0.685645, weighted_f1:0.099156, ratio:0.144617, Precision:0.615552, Recall:0.773754\n",
    "mode:2,  F1:0.901384, weighted_f1:0.282426, ratio:0.313324, Precision:0.850406, Recall:0.958864\n",
    "mode:3,  F1:0.103310, weighted_f1:0.004607, ratio:0.044598, Precision:0.411330, Recall:0.059073\n",
    "mode:4,  F1:0.015228, weighted_f1:0.000372, ratio:0.024453, Precision:0.461538, Recall:0.007742\n",
    "mode:5,  F1:0.841895, weighted_f1:0.082147, ratio:0.097574, Precision:0.774548, Recall:0.922070\n",
    "mode:6,  F1:0.174896, weighted_f1:0.003479, ratio:0.019893, Precision:0.350000, Recall:0.116574\n",
    "mode:7,  F1:0.789026, weighted_f1:0.140384, ratio:0.177920, Precision:0.704656, Recall:0.896347\n",
    "mode:8,  F1:0.263048, weighted_f1:0.001199, ratio:0.004559, Precision:0.331579, Recall:0.217993\n",
    "mode:9,  F1:0.517820, weighted_f1:0.025855, ratio:0.049931, Precision:0.579132, Recall:0.468246\n",
    "mode:10,  F1:0.556323, weighted_f1:0.015842, ratio:0.028475, Precision:0.487886, Recall:0.647091\n",
    "mode:11,  F1:0.468368, weighted_f1:0.003392, ratio:0.007241, Precision:0.477376, Recall:0.459695\n",
    "0.6892103959970385"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
